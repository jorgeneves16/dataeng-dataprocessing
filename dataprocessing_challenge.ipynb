{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE6GjNCKUzjS21KfKB5jCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgeneves16/dataeng-dataprocessing/blob/main/dataprocessing_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SVHdzO-SsOkR",
        "outputId": "aa02a474-0dee-41b0-8af4-dfadafb9427d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ETL program\n",
            "Running Task - Transform Vehicles\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|bearing|            block_id|current_status|      id| latitude|line_id|longitude|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|      date|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|      5|20250628-64020025...| IN_TRANSIT_TO|44|12059|38.523964|   4426|-8.894337|  4426_0_1|  4426_0|            SCHEDULED|112040000007|5.8333335| 160105|2025-06-28 09:11:56|4426_0_1|600|1000...|2025-06-28|\n",
            "|    234|20250628-64020205...| IN_TRANSIT_TO|44|12642|38.522923|   4471|-8.898623|  4471_0_3|  4471_0|            SCHEDULED|111340000007|      0.0| 160407|2025-06-28 09:12:06|4471_0_3|600|1000...|2025-06-28|\n",
            "|    227|      ESC_SAB_ES1040| IN_TRANSIT_TO| 43|2213|38.630592|   3512|-9.127206|  3512_0_1|  3512_0|            SCHEDULED|      ES1040|3.8888888| 140761|2025-06-28 09:12:00|3512_0_1_0930_095...|2025-06-28|\n",
            "|      0|           2_2317-21|    STOPPED_AT| 41|1233|38.705875|   1608|-9.322247|  1608_1_1|  1608_1|            SCHEDULED|        2317|      0.0| 121221|2025-06-28 09:12:05|1608_1_1_1000_102...|2025-06-28|\n",
            "|     38|           2_2904-21| IN_TRANSIT_TO| 41|1353| 38.82314|   1255|-9.351192|  1255_0_2|  1255_0|            SCHEDULED|        2904|3.6111112| 172018|2025-06-28 09:11:31|1255_0_2_1000_102...|2025-06-28|\n",
            "|    312|           2_2741-21| IN_TRANSIT_TO| 41|1730|38.804825|   1210|-9.329054|  1210_0_2|  1210_0|            SCHEDULED|        2741|1.1111112| 170251|2025-06-28 09:11:45|1210_0_2_0930_095...|2025-06-28|\n",
            "|     50|        4538-21'_001|   INCOMING_AT| 42|2574|38.824696|   2774|-9.143468|  2774_0_2|  2774_0|            SCHEDULED|        4537|4.7222223| 071599|2025-06-28 09:11:45|2774_0_2|2|1|0945...|2025-06-28|\n",
            "|      0|             4716-21|    STOPPED_AT| 42|2038|38.912094|   2006|-9.130623|  2006_0_3|  2006_0|            SCHEDULED|        4716|      0.0| 070145|2025-06-28 09:12:07|2006_0_3|2|1|1000...|2025-06-28|\n",
            "|      3|           2_2216-21| IN_TRANSIT_TO| 41|1804| 38.77726|   1213|-9.296071|  1213_0_1|  1213_0|            SCHEDULED|        2216|      5.0| 170635|2025-06-28 09:11:54|1213_0_1_1000_102...|2025-06-28|\n",
            "|      6|             4515-21|    STOPPED_AT| 42|2535|38.804337|   2523| -9.18705|  2523_0_1|  2523_0|            SCHEDULED|        4516|      0.0| 110485|2025-06-28 09:12:02|2523_0_1|2|1|0955...|2025-06-28|\n",
            "|    229|           2_2020-21| IN_TRANSIT_TO| 41|1126|38.764816|   1514|-9.224292|  1514_0_2|  1514_0|            SCHEDULED|        2020|10.277778| 030692|2025-06-28 09:11:45|1514_0_2_0930_095...|2025-06-28|\n",
            "|    198| ESC_SAB_ES1006'_001| IN_TRANSIT_TO| 43|2362|38.623856|   3113|-9.100418|  3113_0_2|  3113_0|            SCHEDULED|      ES1008| 6.111111| 140051|2025-06-28 09:11:47|3113_0_2_1000_102...|2025-06-28|\n",
            "|    313|           2_2210-21| IN_TRANSIT_TO| 41|1377|38.783566|   1219|-9.305301|  1219_0_1|  1219_0|            SCHEDULED|        2210|7.2222223| 170479|2025-06-28 09:12:05|1219_0_1_0930_095...|2025-06-28|\n",
            "|    208|           2_2715-21| IN_TRANSIT_TO| 41|1384|38.829376|   1242|-9.468674|  1242_0_1|  1242_0|            SCHEDULED|        2715|12.777778| 171459|2025-06-28 09:11:28|1242_0_1_0930_095...|2025-06-28|\n",
            "|    317|      ESC_SAB_ES2050| IN_TRANSIT_TO| 43|2244|38.659714|   3017|-9.176588|  3017_0_1|  3017_0|            SCHEDULED|      ES2052|11.388889| 020526|2025-06-28 09:11:48|3017_0_1_0930_095...|2025-06-28|\n",
            "|     18|           2_2903-21| IN_TRANSIT_TO| 41|1177|38.749935|   1512|-9.247194|  1512_0_1|  1512_0|            SCHEDULED|        2903|      7.5| 172055|2025-06-28 09:11:35|1512_0_1_1000_102...|2025-06-28|\n",
            "|    210|             4082-21|    STOPPED_AT| 42|2376| 38.83149|   2730|-9.101304|  2730_0_2|  2730_0|            SCHEDULED|        4082|0.2777778| 071229|2025-06-28 09:12:05|2730_0_2|2|1|1000...|2025-06-28|\n",
            "|    184|           2_2205-21| IN_TRANSIT_TO| 41|1115| 38.77686|   1715|-9.295001|  1715_0_2|  1715_0|            SCHEDULED|        2223| 9.722222| 170637|2025-06-28 09:12:06|1715_0_2_1000_102...|2025-06-28|\n",
            "|     47|20250628-64020183...| IN_TRANSIT_TO|44|12061| 38.52882|   4441|-8.886052|  4441_0_1|  4441_0|            SCHEDULED|112010000007|0.5555556| 160031|2025-06-28 09:11:54|4441_0_1|600|1000...|2025-06-28|\n",
            "|    262|20250628-64020025...| IN_TRANSIT_TO|44|12741|38.523464|   4426|-8.895371|  4426_0_2|  4426_0|            SCHEDULED|112040000007|      0.0| 160434|2025-06-28 09:11:57|4426_0_2|600|0920...|2025-06-28|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "from functools import reduce # Import reduce from functools\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, partition_column: str = None, **kwargs) -> None:\n",
        "        if partition_column:\n",
        "          df.coalesce(1).write.mode(\"overwrite\").partitionBy(partition_column).format(format).save(path)\n",
        "        else:\n",
        "          df.coalesce(1).write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "      # schema\n",
        "      lines_schema = StructType([StructField('color', StringType(), True),\n",
        "                                 StructField('facilities', ArrayType(StringType(), True), True),\n",
        "                                 StructField('id', StringType(), True),\n",
        "                                 StructField('localities',ArrayType(StringType(), True), True),\n",
        "                                 StructField('long_name', StringType(), True),\n",
        "                                 StructField('municipalities', ArrayType(StringType(), True), True),\n",
        "                                 StructField('patterns', ArrayType(StringType(), True), True),\n",
        "                                 StructField('routes', ArrayType(StringType(), True), True),\n",
        "                                 StructField('short_name', StringType(), True), StructField('text_color', StringType(), True)])\n",
        "      # ingestion\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema=lines_schema)\n",
        "\n",
        "      #ensure only 1 file is created\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      # load\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/lines\", mode=\"overwrite\")\n",
        "\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema=vehicle_schema)\n",
        "\n",
        "      # create date column\n",
        "      # date(from_unixtime(1732305594))\n",
        "      df = df.withColumn(\"date\", expr(\"date(timestamp)\"))\n",
        "\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/vehicles\", mode=\"overwrite\", partition_column=\"date\")\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "      municipalities_schema = StructType([StructField('id', StringType(), True),\n",
        "                                          StructField('name', StringType(), True),\n",
        "                                          StructField('prefix', StringType(), True),\n",
        "                                          StructField('district_id', StringType(), True),\n",
        "                                          StructField('district_name', StringType(), True),\n",
        "                                          StructField('region_id', StringType(), True),\n",
        "                                          StructField('region_name', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema=municipalities_schema)\n",
        "\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/municipalities\", mode=\"overwrite\")\n",
        "\n",
        "\n",
        "    def transform_vehicles(self):\n",
        "      df = spark.read.format(\"parquet\").load(\"/content/lake/bronze/vehicles\")\n",
        "      df = df.withColumnRenamed(\"lat\", \"latitude\") \\\n",
        "            .withColumnRenamed(\"lon\", \"longitude\")\n",
        "\n",
        "\n",
        "\n",
        "      df.show()\n",
        "\n",
        "\n",
        "\n",
        "    # def cleansing_vehicles(self):\n",
        "    #   df = self.extract_from_file(format=\"parquet\", path=\"/content/lake/bronze/vehicles\")\n",
        "\n",
        "    #   # transformations\n",
        "    #   df = df.withColumn(\"new_column\", lit(\"test\"))\n",
        "    #   df = df.drop_duplicates()\n",
        "\n",
        "    #   self.load(df=df, format=\"parquet\", path=\"/content/lake/silver/vehicles\")\n",
        "\n",
        "    # def enrich(self):\n",
        "    #     pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('ETL Program').getOrCreate()\n",
        "\n",
        "    print(\"Starting ETL program\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    # print(\"Running Task - Ingestion Vehicles\")\n",
        "    # etl.ingestion_vehicles()\n",
        "\n",
        "    # print(\"Running Task - Ingestion Lines\")\n",
        "    # etl.ingestion_lines()\n",
        "\n",
        "    # print(\"Running Task - Ingestion Municipalities\")\n",
        "    # etl.ingestion_municipalities()\n",
        "\n",
        "    print(\"Running Task - Transform Vehicles\")\n",
        "    etl.transform_vehicles()\n",
        "\n",
        "    # print(\"Running Task - Cleansing Vehicles\")\n",
        "    # etl.cleansing_vehicles()\n",
        "\n",
        "    #etl.enrich()\n",
        "\n",
        "    # print(\"ETL program completed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check results\n",
        "spark.read.parquet(\"/content/lake/bronze/vehicles\").show()\n",
        "spark.read.parquet(\"/content/lake/bronze/lines\").show()\n",
        "spark.read.parquet(\"/content/lake/bronze/municipalities\").show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FN7A5s-90EmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}