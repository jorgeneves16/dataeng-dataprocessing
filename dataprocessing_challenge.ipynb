{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbtvCK/GuSwMxP8JApQdIx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgeneves16/dataeng-dataprocessing/blob/main/dataprocessing_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SVHdzO-SsOkR",
        "outputId": "4d2cd27e-59af-4d80-ebfc-6192e5606912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ETL program\n",
            "Running Task - Ingestion Vehicles\n",
            "Running Task - Ingestion Lines\n",
            "Running Task - Ingestion Municipalities\n",
            "Running Task - Cleansing Vehicles\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|bearing|            block_id|current_status|      id| latitude|line_id|longitude|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|      date|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|    247|       VER_DU_VU2103| IN_TRANSIT_TO| 43|2240|38.673637|   3022|-9.170199|  3022_0_2|  3022_0|            SCHEDULED|      VU2189|10.555555| 020290|2025-07-07 21:51:01|3022_0_2_2230_225...|2025-07-07|\n",
            "|     76|20250707-64810286...|    STOPPED_AT|44|12535|38.683693|   4600|-8.953776|  4600_0_2|  4600_0|            SCHEDULED|321920234560|      0.0| 100418|2025-07-07 21:51:07|4600_0_2|1500|220...|2025-07-07|\n",
            "|      0|           7_7323-13|    STOPPED_AT| 41|1394|38.679653|   1613| -9.32778|  1613_0_2|  1613_0|            SCHEDULED|        7351|1.3888888| 050004|2025-07-07 21:50:16|1613_0_2_2230_225...|2025-07-07|\n",
            "|     93|20250707-64810234...| IN_TRANSIT_TO|44|12059|38.525146|   4422|-8.905075|  4422_0_1|  4422_0|            SCHEDULED|312540234560|6.9444447| 160269|2025-07-07 21:51:00|4422_0_1|1500|223...|2025-07-07|\n",
            "|    305|           7_7037-13|    STOPPED_AT| 41|1148|38.757988|   1514|-9.234367|  1514_0_2|  1514_0|            SCHEDULED|        7135|      0.0| 030821|2025-07-07 21:51:06|1514_0_2_2230_225...|2025-07-07|\n",
            "|    255|           7_7304-13| IN_TRANSIT_TO| 41|1379| 38.72372|   1638|-9.337177|  1638_0_1|  1638_0|            SCHEDULED|        7363|      7.5| 050431|2025-07-07 21:50:51|1638_0_1_2230_225...|2025-07-07|\n",
            "|     55|             3136-13| IN_TRANSIT_TO| 42|2220| 38.89364|   2306|-9.045096|  2306_0_1|  2306_0|            SCHEDULED|        3293|      7.5| 180049|2025-07-07 21:51:08|2306_0_1|1|3|2245...|2025-07-07|\n",
            "|     27|20250707-64810006...| IN_TRANSIT_TO|44|12097|38.744183|   4512|-8.969543|  4512_0_2|  4512_0|            SCHEDULED|321900234560| 8.055555| 010083|2025-07-07 21:51:00|4512_0_2|1500|215...|2025-07-07|\n",
            "|    235|20250707-64810261...| IN_TRANSIT_TO|44|12599| 38.56417|   4562|-8.947947|  4562_0_1|  4562_0|            SCHEDULED|311470234560|14.166667| 130129|2025-07-07 21:51:04|4562_0_1|1500|221...|2025-07-07|\n",
            "|    119|              VS_060| IN_TRANSIT_TO| 41|1326|38.723545|   1722|-9.225679|  1722_0_2|  1722_0|            SCHEDULED|     7497+TP| 9.166667| 120016|2025-07-07 21:51:06|1722_0_2_2200_222...|2025-07-07|\n",
            "|    151|20250707-64810310...| IN_TRANSIT_TO|44|12557| 38.65614|   4621|-8.991016|  4621_0_2|  4621_0|            SCHEDULED|321760234560|7.2222223| 090165|2025-07-07 21:50:58|4621_0_2|1500|215...|2025-07-07|\n",
            "|    300|           7_7206-13| IN_TRANSIT_TO| 41|1104| 38.76991|   1715|-9.289146|  1715_0_1|  1715_0|            SCHEDULED|        7248|7.7777777| 170559|2025-07-07 21:51:03|1715_0_1_2200_222...|2025-07-07|\n",
            "|      0|           7_7208-13|    STOPPED_AT| 41|1186|38.750996|   1523|-9.302707|  1523_0_1|  1523_0|            SCHEDULED|        7257|      0.0| 170775|2025-07-07 21:51:08|1523_0_1_2230_225...|2025-07-07|\n",
            "|    220|             3111-13| IN_TRANSIT_TO| 42|2370|38.855495|   2790|-9.071396|  2790_0_1|  2790_0|            SCHEDULED|        3218|5.5555553| 180875|2025-07-07 21:51:05|2790_0_1|1|3|2230...|2025-07-07|\n",
            "|    140|        3067-13'_001| IN_TRANSIT_TO| 42|2415|38.863667|   2538|-9.092879|  2538_0_2|  2538_0|            SCHEDULED|        3084|5.8333335| 180864|2025-07-07 21:50:49|2538_0_2|1|3|2235...|2025-07-07|\n",
            "|    215|       VER_DU_VU3059| IN_TRANSIT_TO|  43|661|   38.464|   3223|-9.098021|  3223_0_1|  3223_0|            SCHEDULED|      VU3092|      0.0| 150051|2025-07-07 21:50:48|3223_0_1_2230_225...|2025-07-07|\n",
            "|    264|           7_7606-13| IN_TRANSIT_TO| 41|1105|38.757404|   1721|-9.259125|  1721_0_1|  1721_0|            SCHEDULED|        7687|7.7777777| 170924|2025-07-07 21:51:02|1721_0_1_2200_222...|2025-07-07|\n",
            "|    180|             3579-13| IN_TRANSIT_TO| 42|2523|38.804073|   2769|-9.157639|  2769_0_2|  2769_0|            SCHEDULED|        3679| 9.722222| 071137|2025-07-07 21:50:51|2769_0_2|1|3|2235...|2025-07-07|\n",
            "|    221|       VER_DU_VU1012| IN_TRANSIT_TO| 43|2086| 38.63704|   3503|-9.155408|  3503_0_1|  3503_0|            SCHEDULED|      VU1159| 8.611111| 140376|2025-07-07 21:50:55|3503_0_1_2230_225...|2025-07-07|\n",
            "|    173|             3047-13| IN_TRANSIT_TO| 42|2329| 38.78708|   2727|-9.099893|  2727_0_2|  2727_0|            SCHEDULED|        3190| 8.888889| 060195|2025-07-07 21:51:03|2727_0_2|1|3|2210...|2025-07-07|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Cleansing Lines\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------+\n",
            "|  color|facilities|  id|          localities|           long_name|      municipalities|            patterns|              routes|short_name|text_color|\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------+\n",
            "|#3D85C6|        []|1104|[Oeiras, Algés, M...|Algés | Circular ...|              [1110]|          [1104_0_3]|            [1104_0]|      1104|   #FFFFFF|\n",
            "|#C61D23|        []|3119|[Pinhal de Cima, ...|Pinhal Conde Cunh...|              [1510]|[3119_0_1, 3119_0_2]|            [3119_0]|      3119|   #FFFFFF|\n",
            "|#C61D23|        []|1117|[Caxias, Oeiras, ...|Caxias (Qta Moura...|              [1110]|[1117_0_1, 1117_0...|[1117_0, 1117_1, ...|      1117|   #FFFFFF|\n",
            "|#C61D23|        []|1733|[Marquês de Pomba...|Marquês Pombal (M...|  [1106, 1110, 1111]|[1733_0_1, 1733_0...|[1733_0, 1733_1, ...|      1733|   #FFFFFF|\n",
            "|#C61D23|        []|2753|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|[2753_0_1, 2753_0_2]|            [2753_0]|      2753|   #FFFFFF|\n",
            "|#C61D23|        []|4308|[Pinhal Novo, Val...|Palmela (Terminal...|              [1508]|[4308_0_2, 4308_0_1]|            [4308_0]|      4308|   #FFFFFF|\n",
            "|#C61D23|        []|4438|           [Setúbal]|Setúbal (Monte Be...|              [1512]|[4438_0_2, 4438_0...|    [4438_0, 4438_1]|      4438|   #FFFFFF|\n",
            "|#C61D23|        []|4452|[Setúbal, Praias ...|Mitrena (Portucel...|              [1512]|[4452_0_1, 4452_0...|    [4452_0, 4452_1]|      4452|   #FFFFFF|\n",
            "|#C61D23|        []|1103|[Algés, Oeiras, Q...|Algés (Estação) -...|              [1110]|[1103_0_1, 1103_0_2]|            [1103_0]|      1103|   #FFFFFF|\n",
            "|#C61D23|        []|2116|  [Mafra, Murgueira]|Encarnação - Mafr...|              [1109]|[2116_0_1, 2116_0_2]|            [2116_0]|      2116|   #FFFFFF|\n",
            "|#FDB71A|        []|2741|[Campo Grande, Lo...|Ericeira (Termina...|  [1106, 1107, 1109]|[2741_1_2, 2741_1...|    [2741_1, 2741_0]|      2741|   #FFFFFF|\n",
            "|#C61D23|        []|3549|[Quinta do Conde,...|Quinta do Conde -...|        [1511, 1510]|[3549_0_1, 3549_0_2]|            [3549_0]|      3549|   #FFFFFF|\n",
            "|#3D85C6|        []|4106|[Moita, Alhos Ved...|Alhos Vedros | Ci...|              [1506]|          [4106_0_3]|            [4106_0]|      4106|   #FFFFFF|\n",
            "|#C61D23|        []|4541|[Algeruz, Brejos ...|Algeruz - Setúbal...|        [1508, 1512]|[4541_0_1, 4541_0_2]|            [4541_0]|      4541|   #FFFFFF|\n",
            "|#C61D23|        []|4550|[Vila Nogueira de...|Palmela (Terminal...|        [1512, 1508]|[4550_0_2, 4550_0_1]|            [4550_0]|      4550|   #FFFFFF|\n",
            "|#C61D23|        []|1208|[Almargem do Bisp...|Almargem Bispo (C...|              [1111]|[1208_0_1, 1208_0_2]|            [1208_0]|      1208|   #FFFFFF|\n",
            "|#C61D23|        []|1631|[Estoril, Cascais...|Estoril (Estação)...|        [1105, 1111]|[1631_0_1, 1631_0_2]|            [1631_0]|      1631|   #FFFFFF|\n",
            "|#C61D23|        []|3527|[Torre da Caparic...|Monte de Caparica...|        [1503, 1510]|[3527_0_1, 3527_0...|    [3527_0, 3527_1]|      3527|   #FFFFFF|\n",
            "|#C61D23|        []|1220|[Cacém, Sintra, T...|Agualva-Cacém (Es...|              [1111]|          [1220_0_3]|            [1220_0]|      1220|   #FFFFFF|\n",
            "|#C61D23|        []|1245|[Portela de Sintr...|Catribana (Largo)...|              [1111]|[1245_0_2, 1245_0_1]|            [1245_0]|      1245|   #FFFFFF|\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Cleansing Municipalities\n",
            "+----+-------------------+------+-----------+-------------+---------+----------------+\n",
            "|  id|               name|prefix|district_id|district_name|region_id|     region_name|\n",
            "+----+-------------------+------+-----------+-------------+---------+----------------+\n",
            "|1107|             Loures|    07|         11|       Lisboa|    PT170|             AML|\n",
            "|1504|           Barreiro|    04|         15|      Setúbal|    PT170|             AML|\n",
            "|1101|           Alenquer|    20|         11|       Lisboa|    PT16B|           Oeste|\n",
            "|1511|           Sesimbra|    15|         15|      Setúbal|    PT170|             AML|\n",
            "|0712|       Vendas Novas|    19|         07|        Évora|    PT187|Alentejo Central|\n",
            "|1102|  Arruda dos Vinhos|    20|         11|       Lisboa|    PT16B|           Oeste|\n",
            "|1510|             Seixal|    14|         15|      Setúbal|    PT170|             AML|\n",
            "|1114|Vila Franca de Xira|    18|         11|       Lisboa|    PT170|             AML|\n",
            "|1506|              Moita|    09|         15|      Setúbal|    PT170|             AML|\n",
            "|1115|            Amadora|    03|         11|       Lisboa|    PT170|             AML|\n",
            "|1512|            Setúbal|    16|         15|      Setúbal|    PT170|             AML|\n",
            "|1109|              Mafra|    08|         11|       Lisboa|    PT170|             AML|\n",
            "|1113|      Torres Vedras|    20|         11|       Lisboa|    PT16B|           Oeste|\n",
            "|1503|             Almada|    02|         15|      Setúbal|    PT170|             AML|\n",
            "|1110|             Oeiras|    12|         11|       Lisboa|    PT170|             AML|\n",
            "|1111|             Sintra|    17|         11|       Lisboa|    PT170|             AML|\n",
            "|1508|            Palmela|    13|         15|      Setúbal|    PT170|             AML|\n",
            "|1105|            Cascais|    05|         11|       Lisboa|    PT170|             AML|\n",
            "|1116|           Odivelas|    11|         11|       Lisboa|    PT170|             AML|\n",
            "|1502|          Alcochete|    01|         15|      Setúbal|    PT170|             AML|\n",
            "+----+-------------------+------+-----------+-------------+---------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Final data enrichment\n",
            "+-------+--------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+--------------------+\n",
            "|bearing|      block_id|current_status|     id| latitude|line_id|longitude|pattern_id|route_id|schedule_relationship| shift_id|    speed|stop_id|          timestamp|             trip_id|      date|           line_name|   municipality_name|\n",
            "+-------+--------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+--------------------+\n",
            "|      0|          1078|    STOPPED_AT|42|1078| 38.92751|   2740|-9.236736|  2740_2_2|  2740_2|            SCHEDULED|    43869|      0.0| 082004|2025-07-07 21:51:04|2740_2_2|110|3|22...|2025-07-07|Lisboa (C. Grande...|     [Mafra, Lisboa]|\n",
            "|      0|     1079'_001|    STOPPED_AT|42|1076|38.828617|   2751|-9.164975|  2751_1_1|  2751_1|            SCHEDULED|    46011|      0.0| 070250|2025-07-07 21:51:09|2751_1_1|110|3|21...|2025-07-07|Ericeira (Termina...|[Lisboa, Odivelas...|\n",
            "|      0|       3039-13|    STOPPED_AT|42|2100|38.810787|   2725|-9.118875|  2725_0_1|  2725_0|            SCHEDULED|     3169|      0.0| 070511|2025-07-07 21:51:05|2725_0_1|1|3|2230...|2025-07-07|Estação Oriente -...|    [Loures, Lisboa]|\n",
            "|      0|       3534-13|    STOPPED_AT| 42|253|38.781445|   2206|-9.193354|  2206_0_2|  2206_0|            SCHEDULED|     3801|      0.0| 110135|2025-07-07 21:50:59|2206_0_2|1|3|2240...|2025-07-07|Caneças (Esc. Sec...|          [Odivelas]|\n",
            "|      0|       3603-13|    STOPPED_AT|42|2597|38.791924|   2212|-9.173147|  2212_0_3|  2212_0|            SCHEDULED|     3659|      0.0| 110094|2025-07-07 21:50:59|2212_0_3|1|3|2230...|2025-07-07|Odivelas (C. Come...|          [Odivelas]|\n",
            "|      0|    77_7933-13|   INCOMING_AT|41|1338| 38.70044|   1998|-9.418569|  1998_0_2|  1998_0|            SCHEDULED|7674+7933|      0.0| 050006|2025-07-07 21:49:49|1998_0_2_2300_232...|2025-07-07|                NULL|                  []|\n",
            "|      0|     7_7017-13|    STOPPED_AT|41|1132| 38.74585|   1506|-9.246069|  1506_0_3|  1506_0|            SCHEDULED|     7137|      0.0| 030311|2025-07-07 21:50:22|1506_0_3_2230_225...|2025-07-07|Amadora (Hospital...|   [Oeiras, Amadora]|\n",
            "|      0|     7_7034-13|    STOPPED_AT|41|1734| 38.74691|   1001|-9.216166|  1001_0_2|  1001_0|            SCHEDULED|     7126|      0.0| 030049|2025-07-07 21:50:53|1001_0_2_2230_225...|2025-07-07|Alfragide (Estr S...|           [Amadora]|\n",
            "|      0|     7_7208-13|    STOPPED_AT|41|1186|38.750996|   1523|-9.302707|  1523_0_1|  1523_0|            SCHEDULED|     7257|      0.0| 170775|2025-07-07 21:51:08|1523_0_1_2230_225...|2025-07-07|Agualva-Cacém (Es...|    [Sintra, Oeiras]|\n",
            "|      0|     7_7323-13|    STOPPED_AT|41|1394|38.679653|   1613| -9.32778|  1613_0_2|  1613_0|            SCHEDULED|     7351|1.3888888| 050004|2025-07-07 21:50:16|1613_0_2_2230_225...|2025-07-07|Agualva-Cacém (Es...|[Oeiras, Cascais,...|\n",
            "|      0|7_7466-13'_002|    STOPPED_AT|41|1289|38.697346|   1120| -9.30624|  1120_0_1|  1120_0|            SCHEDULED|     7498|3.3333333| 120747|2025-07-07 21:51:05|1120_0_1_2230_225...|2025-07-07|Oeiras (Estação N...|            [Oeiras]|\n",
            "|      0|     7_7652-13|    STOPPED_AT|41|1860|38.764748|   1518|-9.259752|  1518_0_1|  1518_0|            SCHEDULED|     7685|      0.0| 170896|2025-07-07 21:51:12|1518_0_1_2230_225...|2025-07-07|Monte Abraão - Re...|   [Sintra, Amadora]|\n",
            "|      0|     7_7760-13|    STOPPED_AT|41|1222| 38.78355|   1625|-9.322076|  1625_0_1|  1625_0|            SCHEDULED|     7832|      0.0| 170071|2025-07-07 21:50:27|1625_0_1_2200_222...|2025-07-07|Cascais (Terminal...|   [Sintra, Cascais]|\n",
            "|      0| VER_DU_VU1009|    STOPPED_AT|43|2209|38.626877|   3509|-9.082178|  3509_0_1|  3509_0|            SCHEDULED|   VU1184|      0.0| 140021|2025-07-07 21:50:51|3509_0_1_2130_215...|2025-07-07|Cacilhas (Termina...|    [Seixal, Almada]|\n",
            "|      0| VER_DU_VU1057|    STOPPED_AT|43|2378| 38.62082|   3509| -9.08255|  3509_0_1|  3509_0|            SCHEDULED|   VU1174|      0.0| 140279|2025-07-07 21:51:00|3509_0_1_2130_215...|2025-07-07|Cacilhas (Termina...|    [Seixal, Almada]|\n",
            "|      0| VER_DU_VU1102|    STOPPED_AT|43|2220| 38.62697|   3509|-9.115267|  3509_0_2|  3509_0|            SCHEDULED|   VU1187|0.2777778| 140345|2025-07-07 21:50:59|3509_0_2_2200_222...|2025-07-07|Cacilhas (Termina...|    [Seixal, Almada]|\n",
            "|      0| VER_DU_VU2013|    STOPPED_AT|43|2288|38.647404|   3022|-9.202346|  3022_0_1|  3022_0|            SCHEDULED|   VU2211|      0.0| 020707|2025-07-07 21:51:07|3022_0_1_2230_225...|2025-07-07|Costa da Caparica...|            [Almada]|\n",
            "|      0| VER_DU_VU2078|    STOPPED_AT|43|2255| 38.65171|   3013|-9.160476|  3013_0_1|  3013_0|            SCHEDULED|   VU2197|      0.0| 020512|2025-07-07 21:51:09|3013_0_1_2230_225...|2025-07-07|Cacilhas (Termina...|            [Almada]|\n",
            "|      0|        VS_045|    STOPPED_AT|42|2204| 38.80368|   2516|-9.166207|  2516_0_3|  2516_0|            SCHEDULED|     3809|0.2777778| 110659|2025-07-07 21:51:07|2516_0_3|1|3|2235...|2025-07-07|Sr. Roubado (Metr...|  [Loures, Odivelas]|\n",
            "|      1|       3083-13|    STOPPED_AT|42|2408| 38.87693|   2538|-9.086441|  2538_0_1|  2538_0|            SCHEDULED|     3223|1.1111112| 180520|2025-07-07 21:51:11|2538_0_1|1|3|2235...|2025-07-07|P.S.I.(Est) - San...|[Vila Franca de X...|\n",
            "+-------+--------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Gold layer data analysis\n",
            "Top 3 municipalities by vehicles routes:\n",
            "+--------------------------+---------------------+\n",
            "|municipality_name_exploded|vehicles_routes_count|\n",
            "+--------------------------+---------------------+\n",
            "|                    Lisboa|                  105|\n",
            "|                    Loures|                   65|\n",
            "|                    Almada|                   53|\n",
            "+--------------------------+---------------------+\n",
            "\n",
            "Top 3 municipalities by average vehicle speed:\n",
            "+-----------------+------------------+\n",
            "|municipality_name|         avg_speed|\n",
            "+-----------------+------------------+\n",
            "|            Mafra|10.370370335049099|\n",
            "|          Setúbal| 9.146825381687709|\n",
            "|           Lisboa| 8.373015861284165|\n",
            "+-----------------+------------------+\n",
            "\n",
            "Closing Spark Session\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "from functools import reduce # Import reduce from functools\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "    def extract_from_file(self, format: str, path: str, **kwargs) -> DataFrame:\n",
        "        df = self.spark.read.format(format).load(path)\n",
        "        return df\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, partition_column: str = None, **kwargs) -> None:\n",
        "        if partition_column:\n",
        "          df.coalesce(1).write.mode(\"overwrite\").partitionBy(partition_column).format(format).save(path)\n",
        "        else:\n",
        "          df.coalesce(1).write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, partition_column: str = None, dynamic_partition_overwrite: bool = False, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        If 'partition_column' is provided, partition the data accordingly.\n",
        "        If 'dynamic_partition_overwrite' is True, only overwrite the partitions present in the DataFrame.\n",
        "        \"\"\"\n",
        "        # Set Spark configuration for dynamic/static partition overwrite mode\n",
        "        if dynamic_partition_overwrite:\n",
        "            self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
        "        else:\n",
        "            self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"static\")\n",
        "\n",
        "        # Write the DataFrame as a single parquet file, with optional partitioning\n",
        "        if partition_column:\n",
        "            df.coalesce(1).write.mode(\"overwrite\").partitionBy(partition_column).format(format).save(path)\n",
        "        else:\n",
        "            df.coalesce(1).write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "      # schema\n",
        "      lines_schema = StructType([StructField('color', StringType(), True),\n",
        "                                 StructField('facilities', ArrayType(StringType(), True), True),\n",
        "                                 StructField('id', StringType(), True),\n",
        "                                 StructField('localities',ArrayType(StringType(), True), True),\n",
        "                                 StructField('long_name', StringType(), True),\n",
        "                                 StructField('municipalities', ArrayType(StringType(), True), True),\n",
        "                                 StructField('patterns', ArrayType(StringType(), True), True),\n",
        "                                 StructField('routes', ArrayType(StringType(), True), True),\n",
        "                                 StructField('short_name', StringType(), True), StructField('text_color', StringType(), True)])\n",
        "      # ingestion\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema=lines_schema)\n",
        "\n",
        "      #ensure only 1 file is created\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      # load\n",
        "      self.load(df=df, format=\"parquet\", path=\"content/lake/bronze/lines\")\n",
        "\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema=vehicle_schema)\n",
        "\n",
        "      df = df.withColumn(\"date\", expr(\"date(timestamp)\"))\n",
        "\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      self.load(df=df, format=\"parquet\", path=\"content/lake/bronze/vehicles\", partition_column=\"date\", dynamic_partition_overwrite=True)\n",
        "\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "      municipalities_schema = StructType([StructField('id', StringType(), True),\n",
        "                                          StructField('name', StringType(), True),\n",
        "                                          StructField('prefix', StringType(), True),\n",
        "                                          StructField('district_id', StringType(), True),\n",
        "                                          StructField('district_name', StringType(), True),\n",
        "                                          StructField('region_id', StringType(), True),\n",
        "                                          StructField('region_name', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema=municipalities_schema)\n",
        "\n",
        "      df = df.repartition(1)\n",
        "\n",
        "      self.load(df=df, format=\"parquet\", path=\"content/lake/bronze/municipalities\")\n",
        "\n",
        "\n",
        "    def cleansing_vehicles(self):\n",
        "        df = self.extract_from_file(format=\"parquet\", path=\"content/lake/bronze/vehicles\")\n",
        "\n",
        "        df = df.withColumnRenamed(\"lat\", \"latitude\") \\\n",
        "              .withColumnRenamed(\"lon\", \"longitude\")\n",
        "\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        df = df.filter(\n",
        "            col(\"current_status\").isNotNull() &\n",
        "            (col(\"current_status\") != \"\") &\n",
        "            (col(\"current_status\") != \"NONE\")\n",
        "        )\n",
        "\n",
        "        df = df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
        "\n",
        "        df.show()\n",
        "\n",
        "        self.load(df=df, format=\"parquet\", path=\"content/lake/silver/vehicles\", partition_column=\"date\", dynamic_partition_overwrite=True)\n",
        "\n",
        "\n",
        "\n",
        "    def cleansing_lines(self):\n",
        "        df = self.extract_from_file(format=\"parquet\", path=\"content/lake/bronze/lines\")\n",
        "\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        df = df.filter(\n",
        "            col(\"long_name\").isNotNull() &\n",
        "            (col(\"long_name\") != \"\") &\n",
        "            (col(\"long_name\") != \"NONE\")\n",
        "        )\n",
        "\n",
        "        df.show()\n",
        "\n",
        "        self.load(df=df, format=\"parquet\", path=\"content/lake/silver/lines\")\n",
        "\n",
        "\n",
        "    def cleansing_municipalities(self):\n",
        "        df = self.extract_from_file(format=\"parquet\", path=\"content/lake/bronze/municipalities\")\n",
        "\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        df = df.filter(\n",
        "            col(\"name\").isNotNull() &\n",
        "            (col(\"name\") != \"\") &\n",
        "            (col(\"name\") != \"NONE\") &\n",
        "            col(\"district_name\").isNotNull() &\n",
        "            (col(\"district_name\") != \"\") &\n",
        "            (col(\"district_name\") != \"NONE\")\n",
        "        )\n",
        "\n",
        "        df.show()\n",
        "\n",
        "        self.load(df=df, format=\"parquet\", path=\"content/lake/silver/municipalities\")\n",
        "\n",
        "\n",
        "    def enrich_vehicles(self):\n",
        "        df_vehicles = self.extract_from_file(format=\"parquet\", path=\"content/lake/silver/vehicles\")\n",
        "\n",
        "        df_lines = self.extract_from_file(format=\"parquet\", path=\"content/lake/silver/lines\")\n",
        "\n",
        "        df_municipalities = self.extract_from_file(format=\"parquet\", path=\"content/lake/silver/municipalities\")\n",
        "\n",
        "        #Explode municipalities array from lines to create one row per municipality\n",
        "        df_lines_exploded = df_lines.select(\n",
        "            F.col(\"id\").alias(\"line_id_from_lines\"),\n",
        "            F.col(\"long_name\").alias(\"line_name\"),\n",
        "            F.explode_outer(\"municipalities\").alias(\"municipality_id\"),\n",
        "        )\n",
        "\n",
        "        #Join vehicles with lines exploded (left join on line_id)\n",
        "        df_vehicles_lines = df_vehicles.join(\n",
        "            df_lines_exploded,\n",
        "            df_vehicles[\"line_id\"] == df_lines_exploded[\"line_id_from_lines\"],\n",
        "            how=\"left\"\n",
        "        ).drop(\"line_id_from_lines\")\n",
        "\n",
        "        df_municipalities_selected = df_municipalities.select(\n",
        "            F.col(\"id\").alias(\"municipality_id\"),\n",
        "            F.col(\"name\")\n",
        "        )\n",
        "\n",
        "        #Join with municipalities to get municipality name\n",
        "        df_enriched = df_vehicles_lines.join(\n",
        "            df_municipalities_selected,\n",
        "            df_vehicles_lines[\"municipality_id\"] == df_municipalities_selected[\"municipality_id\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        #Select all vehicle columns + line_name + municipality name\n",
        "        vehicle_columns = [col for col in df_vehicles.columns]\n",
        "        df_selected = df_enriched.select(\n",
        "            *vehicle_columns,\n",
        "            F.col(\"line_name\"),\n",
        "            F.col(\"name\")\n",
        "        )\n",
        "\n",
        "        #Group by all vehicle columns + line_name and collect municipality name into an array\n",
        "        grouped_columns = vehicle_columns + ['line_name']\n",
        "        df_enriched_final = df_selected.groupBy(grouped_columns).agg(\n",
        "            F.collect_list(\"name\").alias(\"municipality_name\")\n",
        "        )\n",
        "\n",
        "        #Remove duplicates from municipality_name array and handle null values\n",
        "        df_enriched_final = df_enriched_final.withColumn(\n",
        "            \"municipality_name\",\n",
        "            F.when(\n",
        "                F.col(\"municipality_name\").isNull(), F.array()\n",
        "            ).when(\n",
        "                F.size(F.col(\"municipality_name\")) == 0, F.array()\n",
        "            ).otherwise(\n",
        "                F.array_distinct(F.col(\"municipality_name\"))\n",
        "            )\n",
        "        )\n",
        "\n",
        "        df_enriched_final.show()\n",
        "\n",
        "        self.load(df=df_enriched_final, format=\"parquet\", path=\"content/lake/gold/vehicles_enriched\", partition_column=\"date\", dynamic_partition_overwrite=True)\n",
        "\n",
        "\n",
        "    def gold_layer_analysis(self):\n",
        "      df = self.extract_from_file(format=\"parquet\", path=\"content/lake/gold/vehicles_enriched\")\n",
        "\n",
        "      #Question 1- What are the top 3 municipalities by vehicles routes?\n",
        "      print(\"Top 3 municipalities by vehicles routes:\")\n",
        "      df1 = df.withColumn(\"municipality_name_exploded\", F.explode(\"municipality_name\")) \\\n",
        "        .groupby('municipality_name_exploded') \\\n",
        "        .agg(F.count('municipality_name_exploded').alias('vehicles_routes_count')) \\\n",
        "        .orderBy(F.desc('vehicles_routes_count')) \\\n",
        "        .limit(3) \\\n",
        "        .show()\n",
        "\n",
        "      #Question 2- What are the top 3 municipalities with higher vehicles speed on average?\n",
        "      df.createOrReplaceTempView(\"temp_vehicles_enriched\")\n",
        "\n",
        "      query = \"\"\"\n",
        "      SELECT\n",
        "        exploded_municipality AS municipality_name,\n",
        "        AVG(speed) as avg_speed\n",
        "      FROM (\n",
        "        SELECT EXPLODE (municipality_name) AS exploded_municipality, speed\n",
        "        FROM temp_vehicles_enriched\n",
        "      )\n",
        "      GROUP BY exploded_municipality\n",
        "      ORDER BY avg_speed DESC\n",
        "      LIMIT 3\n",
        "      \"\"\"\n",
        "\n",
        "      df2 = spark.sql(query)\n",
        "      print(\"Top 3 municipalities by average vehicle speed:\")\n",
        "      df2.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('ETL Program').getOrCreate()\n",
        "\n",
        "    try:\n",
        "      print(\"Starting ETL program\")\n",
        "      etl = ETLTask(spark)\n",
        "\n",
        "      # run tasks\n",
        "      print(\"Running Task - Ingestion Vehicles\")\n",
        "      etl.ingestion_vehicles()\n",
        "\n",
        "      print(\"Running Task - Ingestion Lines\")\n",
        "      etl.ingestion_lines()\n",
        "\n",
        "      print(\"Running Task - Ingestion Municipalities\")\n",
        "      etl.ingestion_municipalities()\n",
        "\n",
        "      print(\"Running Task - Cleansing Vehicles\")\n",
        "      etl.cleansing_vehicles()\n",
        "\n",
        "      print(\"Running Task - Cleansing Lines\")\n",
        "      etl.cleansing_lines()\n",
        "\n",
        "      print(\"Running Task - Cleansing Municipalities\")\n",
        "      etl.cleansing_municipalities()\n",
        "\n",
        "      print(\"Running Task - Final data enrichment\")\n",
        "      etl.enrich_vehicles()\n",
        "\n",
        "      print(\"Running Task - Gold layer data analysis\")\n",
        "      etl.gold_layer_analysis()\n",
        "\n",
        "    finally:\n",
        "      print(\"Closing Spark Session\")\n",
        "      spark.stop()"
      ]
    }
  ]
}