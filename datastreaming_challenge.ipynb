{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgeneves16/dataeng-dataprocessing/blob/main/datastreaming_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uYXeODL0T1fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80801ca-3092-4787-faf5-c8810811c27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Udk3tohSaXOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122ab236-8173-4753-c0e2-5fd2f8845e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.4.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "HV7j0eHsDElN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe62df1-7826-430e-9098-b8a7feefa0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old data deleted with success.\n",
            "Event producing ended.\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "\n",
        "\n",
        "# Delete old directory data\n",
        "import shutil\n",
        "import os\n",
        "base_path = \"content/lake/bronze/messages/\"\n",
        "if os.path.exists(base_path):\n",
        "    shutil.rmtree(base_path)\n",
        "    print(\"Old data deleted with success.\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n",
        "print(\"Event producing ended.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248e0d8d-c427-4f70-d1a7-2c33f8521644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2025-07-05 15:51:...|   56|  RECEIVED|6b72f7a0-3c44-4f3...|  EMAIL|      2014|   1007|\n",
            "|2025-07-05 15:50:...|   15|  RECEIVED|ca32dc64-df28-4ee...|  EMAIL|      2008|   1001|\n",
            "|2025-07-05 15:51:...|   36|  RECEIVED|d55eb53b-aaa8-4bc...|  OTHER|      2013|   1005|\n",
            "|2025-07-05 15:51:...|   37|  RECEIVED|efed998a-3942-45c...|  OTHER|      2001|   1018|\n",
            "|2025-07-05 15:50:...|   33|   CLICKED|59718427-85a4-497...|  EMAIL|      2013|   1001|\n",
            "|2025-07-05 15:51:...|   54|   CREATED|8a510b75-fe07-428...|  EMAIL|      2003|   1031|\n",
            "|2025-07-05 15:50:...|   14|   CLICKED|68e7ca36-983e-434...|  EMAIL|      2015|   1046|\n",
            "|2025-07-05 15:50:...|   20|  RECEIVED|00becc4e-09fb-4d4...|   PUSH|      2014|   1046|\n",
            "|2025-07-05 15:51:...|   52|   CLICKED|9a2df0b5-425f-4c8...|  EMAIL|      2013|   1007|\n",
            "|2025-07-05 15:50:...|   16|   CREATED|c8aaef9b-0337-4bf...|  EMAIL|      2013|   1042|\n",
            "|2025-07-05 15:51:...|   46|  RECEIVED|34e75920-2f97-41e...|   PUSH|      2004|   1002|\n",
            "|2025-07-05 15:50:...|   13|   CREATED|c57c838c-fb15-46f...|  EMAIL|      2005|   1029|\n",
            "|2025-07-05 15:50:...|    4|   CLICKED|d62424bf-bd50-422...|  OTHER|      2012|   1034|\n",
            "|2025-07-05 15:51:...|   40|  RECEIVED|bed22fc3-ed7b-468...|   PUSH|      2005|   1027|\n",
            "|2025-07-05 15:50:...|    3|   CLICKED|d62424bf-bd50-422...|  OTHER|      2012|   1034|\n",
            "|2025-07-05 15:51:...|   48|   CLICKED|5e37c028-c3ad-457...|   PUSH|      2013|   1046|\n",
            "|2025-07-05 15:50:...|   27|  RECEIVED|bed22fc3-ed7b-468...|    SMS|      2000|   1010|\n",
            "|2025-07-05 15:51:...|   44|   CREATED|c88fb54e-11af-46f...|   CHAT|      2010|   1024|\n",
            "|2025-07-05 15:50:...|   31|   CLICKED|09d4b78a-d903-49f...|   PUSH|      2006|   1007|\n",
            "|2025-07-05 15:50:...|   17|   CLICKED|9b50ee06-1bbb-4f9...|   CHAT|      2015|   1028|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/\")\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import col, to_date\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# 1. Spark Session\n",
        "def create_spark_session(app_name=\"Streaming Job\"):\n",
        "    return SparkSession.builder.master(\"local\").appName(app_name).getOrCreate()\n",
        "\n",
        "# 2. Schema definition\n",
        "def get_message_schema():\n",
        "    return StructType([\n",
        "        StructField(\"timestamp\", TimestampType(), True),\n",
        "        StructField(\"event_type\", StringType(), True),\n",
        "        StructField(\"message_id\", StringType(), True),\n",
        "        StructField(\"channel\", StringType(), True),\n",
        "        StructField(\"country_id\", IntegerType(), True),\n",
        "        StructField(\"user_id\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "# 3. Read stream from bronze\n",
        "def read_bronze_stream(spark: SparkSession, schema: StructType, path: str) -> DataFrame:\n",
        "    return (spark.readStream\n",
        "        .schema(schema)\n",
        "        .format(\"parquet\")\n",
        "        .load(path)\n",
        "    )\n",
        "\n",
        "# 4. Filter logic\n",
        "def filter_messages(df: DataFrame, is_corrupted=True) -> DataFrame:\n",
        "    if is_corrupted:\n",
        "        return df.filter(\n",
        "            col(\"event_type\").isNull() |\n",
        "            (col(\"event_type\") == \"\") |\n",
        "            (col(\"event_type\") == \"NONE\")\n",
        "        )\n",
        "    else:\n",
        "        return df.filter(\n",
        "            col(\"event_type\").isNotNull() &\n",
        "            (col(\"event_type\") != \"\") &\n",
        "            (col(\"event_type\") != \"NONE\")\n",
        "        )\n",
        "\n",
        "# 5. Join with countries and add partition date\n",
        "def enrich_and_partition(df: DataFrame, countries_df: DataFrame) -> DataFrame:\n",
        "    return (df.join(countries_df, on=\"country_id\", how=\"left\")\n",
        "             .withColumn(\"date\", to_date(col(\"timestamp\"))))\n",
        "\n",
        "# 6. Write stream to silver\n",
        "def write_to_silver(df: DataFrame, path: str, checkpoint_path: str):\n",
        "    return (df.writeStream\n",
        "        .format(\"parquet\")\n",
        "        .option(\"path\", path)\n",
        "        .option(\"checkpointLocation\", checkpoint_path)\n",
        "        .partitionBy(\"date\")\n",
        "        .outputMode(\"append\")\n",
        "        .trigger(processingTime=\"5 seconds\")\n",
        "        .start()\n",
        "    )\n",
        "\n",
        "\n",
        "# Setup\n",
        "spark = create_spark_session(\"Messages Processor\")\n",
        "schema = get_message_schema()\n",
        "\n",
        "# Read of bronze layer data\n",
        "df_stream = read_bronze_stream(spark, schema, \"content/lake/bronze/messages/data/\")\n",
        "\n",
        "# Valid messages:\n",
        "df_valid = filter_messages(df_stream, is_corrupted=False)\n",
        "df_valid_enriched = enrich_and_partition(df_valid, countries)\n",
        "query_valid = write_to_silver(\n",
        "    df_valid_enriched,\n",
        "    \"content/lake/silver/messages/data\",\n",
        "    \"content/lake/silver/messages/checkpoint\"\n",
        ")\n",
        "\n",
        "# Corrupted messages:\n",
        "df_corrupted = filter_messages(df_stream, is_corrupted=True)\n",
        "df_corrupted_enriched = enrich_and_partition(df_corrupted, countries)\n",
        "query_corrupted = write_to_silver(\n",
        "    df_corrupted_enriched,\n",
        "    \"content/lake/silver/messages_corrupted/data\",\n",
        "    \"content/lake/silver/messages_corrupted/checkpoint\"\n",
        ")\n",
        "\n",
        "query_valid.awaitTermination(20)\n",
        "query_corrupted.awaitTermination(20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neriGId-MFNo",
        "outputId": "bc034399-f539-437a-f32e-55cb0c242ff4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "H5KjNzv76ZU4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete old directory data - auxiliary code for development purpose\n",
        "import shutil\n",
        "import os\n",
        "base_path = \"content/lake/silver/\"\n",
        "if os.path.exists(base_path):\n",
        "    shutil.rmtree(base_path)"
      ],
      "metadata": {
        "id": "OS-gvg14GK2L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63774356-f182-4253-db14-15dfb5a0436f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bronze count: 58\n",
            "Silver valid count: 40\n",
            "Silver corrupted count: 18\n",
            "Total Silver count: 58\n",
            "Valid data: bronze == silver valid + silver corrupted\n"
          ]
        }
      ],
      "source": [
        "df_bronze = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/\")\n",
        "bronze_count = df_bronze.count()\n",
        "print(\"Bronze count:\", bronze_count)\n",
        "\n",
        "df_valid = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/\")\n",
        "valid_count = df_valid.count()\n",
        "print(\"Silver valid count:\", valid_count)\n",
        "\n",
        "df_corrupted = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data/\")\n",
        "corrupted_count = df_corrupted.count()\n",
        "print(\"Silver corrupted count:\", corrupted_count)\n",
        "\n",
        "total_silver = valid_count + corrupted_count\n",
        "print(\"Total Silver count:\", total_silver)\n",
        "\n",
        "if bronze_count == total_silver:\n",
        "    print(\"Valid data: bronze == silver valid + silver corrupted\")\n",
        "else:\n",
        "    print(\"Inconsistent: silver total data is not consistent with bronze data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Read parquet data into a DataFrame\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/\")\n",
        "\n",
        "# Define window specification:\n",
        "# Partition by the columns that define duplicates (excluding timestamp),\n",
        "# and order by timestamp ascending to get the oldest record first\n",
        "window_spec = Window.partitionBy(\"message_id\", \"channel\", \"event_type\").orderBy(col(\"timestamp\").asc())\n",
        "\n",
        "# Add a row number column 'rn' to identify the oldest record within each partition\n",
        "df_ranked = df.withColumn(\"rn\", row_number().over(window_spec))\n",
        "\n",
        "# Filter to keep only the first record (oldest timestamp) per group and drop the helper column\n",
        "df_dedup = df_ranked.filter(col(\"rn\") == 1).drop(\"rn\")\n",
        "\n",
        "# Show the deduplicated results\n",
        "df_dedup.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w952WsbsOlqA",
        "outputId": "cdba202d-d57a-44f3-ac4b-067ca21805b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "|country_id|           timestamp|event_type|          message_id|channel|user_id|       country|      date|\n",
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "|      2005|2025-07-05 15:50:...|   CLICKED|0053f08d-dee5-4ef...|   CHAT|   1050|         Italy|2025-07-05|\n",
            "|      2006|2025-07-05 15:50:...|      SENT|00becc4e-09fb-4d4...|  OTHER|   1009|United Kingdom|2025-07-05|\n",
            "|      2014|2025-07-05 15:50:...|  RECEIVED|00becc4e-09fb-4d4...|   PUSH|   1046|        Russia|2025-07-05|\n",
            "|      2006|2025-07-05 15:50:...|   CLICKED|09d4b78a-d903-49f...|   PUSH|   1007|United Kingdom|2025-07-05|\n",
            "|      2007|2025-07-05 15:50:...|      OPEN|09d4b78a-d903-49f...|    SMS|   1050| United States|2025-07-05|\n",
            "|      2006|2025-07-05 15:51:...|   CREATED|0b2c3c09-0144-41b...|   PUSH|   1041|United Kingdom|2025-07-05|\n",
            "|      2015|2025-07-05 15:51:...|   CLICKED|0bc18c35-a5cb-4aa...|    SMS|   1014|     Argentina|2025-07-05|\n",
            "|      2006|2025-07-05 15:50:...|  RECEIVED|0bc18c35-a5cb-4aa...|    SMS|   1041|United Kingdom|2025-07-05|\n",
            "|      2002|2025-07-05 15:50:...|      SENT|31894822-e47d-4e1...|  EMAIL|   1015|         Spain|2025-07-05|\n",
            "|      2007|2025-07-05 15:50:...|      SENT|31894822-e47d-4e1...|   PUSH|   1016| United States|2025-07-05|\n",
            "|      2015|2025-07-05 15:50:...|   CREATED|31894822-e47d-4e1...|    SMS|   1018|     Argentina|2025-07-05|\n",
            "|      2012|2025-07-05 15:50:...|      SENT|34e75920-2f97-41e...|   CHAT|   1049|         India|2025-07-05|\n",
            "|      2004|2025-07-05 15:51:...|  RECEIVED|34e75920-2f97-41e...|   PUSH|   1002|        France|2025-07-05|\n",
            "|      2002|2025-07-05 15:51:...|   CREATED|51dbec4d-da18-48b...|    SMS|   1027|         Spain|2025-07-05|\n",
            "|      2011|2025-07-05 15:51:...|      OPEN|552a6919-0afd-4ab...|  OTHER|   1036|         China|2025-07-05|\n",
            "|      2013|2025-07-05 15:50:...|   CLICKED|59718427-85a4-497...|  EMAIL|   1001|   South Korea|2025-07-05|\n",
            "|      2006|2025-07-05 15:51:...|      SENT|5e37c028-c3ad-457...|  OTHER|   1027|United Kingdom|2025-07-05|\n",
            "|      2013|2025-07-05 15:51:...|   CLICKED|5e37c028-c3ad-457...|   PUSH|   1046|   South Korea|2025-07-05|\n",
            "|      2015|2025-07-05 15:50:...|   CLICKED|68e7ca36-983e-434...|  EMAIL|   1046|     Argentina|2025-07-05|\n",
            "|      2014|2025-07-05 15:51:...|  RECEIVED|6b72f7a0-3c44-4f3...|  EMAIL|   1007|        Russia|2025-07-05|\n",
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955b4669-2d75-4eb4-8e9f-6b941cad4914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2025-07-05|    SMS|      1|      3|   2|       2|   1|\n",
            "|2025-07-05|   PUSH|      2|      1|NULL|       3|   2|\n",
            "|2025-07-05|   CHAT|      3|      1|NULL|    NULL|   1|\n",
            "|2025-07-05|  OTHER|      1|   NULL|   1|       2|   2|\n",
            "|2025-07-05|  EMAIL|      3|      3|NULL|       2|   1|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Report 1\n",
        "df_dedup.groupBy(\"date\", \"channel\").pivot(\"event_type\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b225e53-b33c-43c5-8d99-775e248eed5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|   1016|         1|   0|    0|    0|   1|  0|\n",
            "|   1005|         1|   0|    0|    1|   0|  0|\n",
            "|   1031|         1|   0|    1|    0|   0|  0|\n",
            "|   1034|         1|   0|    0|    1|   0|  0|\n",
            "|   1046|         3|   0|    1|    0|   2|  0|\n",
            "|   1021|         1|   0|    0|    0|   1|  0|\n",
            "|   1028|         1|   1|    0|    0|   0|  0|\n",
            "|   1029|         1|   0|    1|    0|   0|  0|\n",
            "|   1010|         1|   0|    0|    0|   0|  1|\n",
            "|   1002|         1|   0|    0|    0|   1|  0|\n",
            "|   1050|         2|   1|    0|    0|   0|  1|\n",
            "|   1045|         1|   0|    0|    0|   0|  1|\n",
            "|   1036|         1|   0|    0|    1|   0|  0|\n",
            "|   1015|         1|   0|    1|    0|   0|  0|\n",
            "|   1041|         2|   0|    0|    0|   1|  1|\n",
            "|   1001|         2|   0|    2|    0|   0|  0|\n",
            "|   1007|         3|   0|    2|    0|   1|  0|\n",
            "|   1049|         1|   1|    0|    0|   0|  0|\n",
            "|   1042|         1|   0|    1|    0|   0|  0|\n",
            "|   1014|         2|   0|    0|    0|   0|  2|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Report 2\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "# Pivot the channels per user\n",
        "df_pivot = df_dedup.groupBy(\"user_id\").pivot(\"channel\").count().na.fill(0)\n",
        "\n",
        "# Add 'iterations' column (sum of all channel counts)\n",
        "channel_cols = df_pivot.columns[1:]\n",
        "\n",
        "df_result = df_pivot.withColumn(\n",
        "    \"iterations\", sum([col(c) for c in channel_cols])\n",
        ")\n",
        "\n",
        "df_result = df_result.select([\"user_id\", \"iterations\"] + channel_cols)\n",
        "\n",
        "df_result.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}