{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgeneves16/dataeng-dataprocessing/blob/main/datastreaming_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXeODL0T1fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80801ca-3092-4787-faf5-c8810811c27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Udk3tohSaXOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef634da-b8c6-4267-8312-3cf9cee9b680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.4.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "HV7j0eHsDElN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPCOdivrfhYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028679f9-a90e-4a61-d178-5a43a51f4a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old data deleted with success.\n",
            "Event producing ended.\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "\n",
        "\n",
        "# Delete old directory data\n",
        "base_path = \"content/lake/bronze/messages/\"\n",
        "if os.path.exists(base_path):\n",
        "    shutil.rmtree(base_path)\n",
        "    print(\"Bronze layer old data deleted with success.\")\n",
        "\n",
        "# Delete old directory data from Silver layer\n",
        "base_path_silver = \"content/lake/silver/\"\n",
        "if os.path.exists(base_path_silver):\n",
        "    shutil.rmtree(base_path_silver)\n",
        "    print(\"Silver layer old data deleted with success.\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n",
        "print(\"Event producing ended.\")\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2a92cd-742a-4b79-d82a-f27c4ac79ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2025-07-16 19:13:...|    0|   CREATED|ce280da9-cc64-4e9...|   CHAT|      2004|   1005|\n",
            "|2025-07-16 19:13:...|    2|   CREATED|ce280da9-cc64-4e9...|   CHAT|      2004|   1005|\n",
            "|2025-07-16 19:13:...|    4|   CREATED|ce280da9-cc64-4e9...|   CHAT|      2004|   1005|\n",
            "|2025-07-16 19:13:...|    1|   CREATED|ce280da9-cc64-4e9...|   CHAT|      2004|   1005|\n",
            "|2025-07-16 19:13:...|    3|   CREATED|ce280da9-cc64-4e9...|   CHAT|      2004|   1005|\n",
            "|2025-07-16 19:14:...|   63|  RECEIVED|d5add2df-f26a-47a...|  EMAIL|      2015|   1050|\n",
            "|2025-07-16 19:14:...|   36|  RECEIVED|f33acb4b-8490-492...|  EMAIL|      2012|   1021|\n",
            "|2025-07-16 19:13:...|    5|  RECEIVED|c9a3e45c-83f7-455...|   PUSH|      2002|   1035|\n",
            "|2025-07-16 19:14:...|   19|  RECEIVED|1031e78f-cd15-46c...|   CHAT|      2009|   1022|\n",
            "|2025-07-16 19:14:...|   47|  RECEIVED|92db7f0f-1494-47a...|   PUSH|      2012|   1012|\n",
            "|2025-07-16 19:14:...|   15|  RECEIVED|91a70ffb-03d3-422...|   CHAT|      2010|   1013|\n",
            "|2025-07-16 19:14:...|   56|  RECEIVED|b1837c12-cd26-4b1...|   PUSH|      2014|   1021|\n",
            "|2025-07-16 19:14:...|   23|   CREATED|db14ed1a-160d-49e...|  EMAIL|      2007|   1029|\n",
            "|2025-07-16 19:13:...|    6|  RECEIVED|c9a3e45c-83f7-455...|   PUSH|      2002|   1035|\n",
            "|2025-07-16 19:14:...|   48|   CREATED|26938e6c-ced9-43f...|  EMAIL|      2013|   1032|\n",
            "|2025-07-16 19:13:...|    8|   CLICKED|fd8f7a8f-a4c3-4f8...|  OTHER|      2009|   1018|\n",
            "|2025-07-16 19:14:...|   62|   CLICKED|cc1ce419-58df-423...|  OTHER|      2001|   1039|\n",
            "|2025-07-16 19:14:...|   28|   CLICKED|c6c9571b-d9c7-467...|  EMAIL|      2006|   1041|\n",
            "|2025-07-16 19:14:...|   22|  RECEIVED|d3474f19-51c2-474...|    SMS|      2007|   1015|\n",
            "|2025-07-16 19:14:...|   65|   CLICKED|bd83b249-c123-443...|   CHAT|      2012|   1017|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/\")\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import col, to_date\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# 1. Spark Session\n",
        "def create_spark_session(app_name=\"Streaming Job\"):\n",
        "    return SparkSession.builder.master(\"local\").appName(app_name).getOrCreate()\n",
        "\n",
        "# 2. Schema definition\n",
        "def get_message_schema():\n",
        "    return StructType([\n",
        "        StructField(\"timestamp\", TimestampType(), True),\n",
        "        StructField(\"event_type\", StringType(), True),\n",
        "        StructField(\"message_id\", StringType(), True),\n",
        "        StructField(\"channel\", StringType(), True),\n",
        "        StructField(\"country_id\", IntegerType(), True),\n",
        "        StructField(\"user_id\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "# 3. Read stream from bronze\n",
        "def read_bronze_stream(spark: SparkSession, schema: StructType, path: str) -> DataFrame:\n",
        "    return (spark.readStream\n",
        "        .schema(schema)\n",
        "        .format(\"parquet\")\n",
        "        .load(path)\n",
        "    )\n",
        "\n",
        "# 4. Filter logic\n",
        "def filter_messages(df: DataFrame, is_corrupted=True) -> DataFrame:\n",
        "    if is_corrupted:\n",
        "        return df.filter(\n",
        "            col(\"event_type\").isNull() |\n",
        "            (col(\"event_type\") == \"\") |\n",
        "            (col(\"event_type\") == \"NONE\")\n",
        "        )\n",
        "    else:\n",
        "        return df.filter(\n",
        "            col(\"event_type\").isNotNull() &\n",
        "            (col(\"event_type\") != \"\") &\n",
        "            (col(\"event_type\") != \"NONE\")\n",
        "        )\n",
        "\n",
        "# 5. Join with countries and add partition date\n",
        "def enrich_and_partition(df: DataFrame, countries_df: DataFrame) -> DataFrame:\n",
        "    return (df.join(countries_df, on=\"country_id\", how=\"left\")\n",
        "             .withColumn(\"date\", to_date(col(\"timestamp\"))))\n",
        "\n",
        "# 6. Write stream to silver\n",
        "def write_to_silver(df: DataFrame, path: str, checkpoint_path: str):\n",
        "    return (df.writeStream\n",
        "        .format(\"parquet\")\n",
        "        .option(\"path\", path)\n",
        "        .option(\"checkpointLocation\", checkpoint_path)\n",
        "        .partitionBy(\"date\")\n",
        "        .outputMode(\"append\")\n",
        "        .trigger(processingTime=\"5 seconds\")\n",
        "        .start()\n",
        "    )\n",
        "\n",
        "\n",
        "# Setup\n",
        "spark = create_spark_session(\"Messages Processor\")\n",
        "schema = get_message_schema()\n",
        "\n",
        "# Read of bronze layer data\n",
        "df_stream = read_bronze_stream(spark, schema, \"content/lake/bronze/messages/data/\")\n",
        "\n",
        "# Valid messages:\n",
        "df_valid = filter_messages(df_stream, is_corrupted=False)\n",
        "df_valid_enriched = enrich_and_partition(df_valid, countries)\n",
        "query_valid = write_to_silver(\n",
        "    df_valid_enriched,\n",
        "    \"content/lake/silver/messages/data\",\n",
        "    \"content/lake/silver/messages/checkpoint\"\n",
        ")\n",
        "\n",
        "# Corrupted messages:\n",
        "df_corrupted = filter_messages(df_stream, is_corrupted=True)\n",
        "df_corrupted_enriched = enrich_and_partition(df_corrupted, countries)\n",
        "query_corrupted = write_to_silver(\n",
        "    df_corrupted_enriched,\n",
        "    \"content/lake/silver/messages_corrupted/data\",\n",
        "    \"content/lake/silver/messages_corrupted/checkpoint\"\n",
        ")\n",
        "\n",
        "query_valid.awaitTermination(20)\n",
        "query_corrupted.awaitTermination(20)\n",
        "\n",
        "query_valid.stop()\n",
        "query_corrupted.stop()\n"
      ],
      "metadata": {
        "id": "neriGId-MFNo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82167404-1eca-4e63-9209-f8c8e1f40032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bronze count: 58\n",
            "Silver valid count: 49\n",
            "Silver corrupted count: 9\n",
            "Total Silver count: 58\n",
            "Valid data: bronze == silver valid + silver corrupted\n"
          ]
        }
      ],
      "source": [
        "df_bronze = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/\")\n",
        "bronze_count = df_bronze.count()\n",
        "print(\"Bronze count:\", bronze_count)\n",
        "\n",
        "df_valid = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/\")\n",
        "valid_count = df_valid.count()\n",
        "print(\"Silver valid count:\", valid_count)\n",
        "\n",
        "df_corrupted = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data/\")\n",
        "corrupted_count = df_corrupted.count()\n",
        "print(\"Silver corrupted count:\", corrupted_count)\n",
        "\n",
        "total_silver = valid_count + corrupted_count\n",
        "print(\"Total Silver count:\", total_silver)\n",
        "\n",
        "if bronze_count == total_silver:\n",
        "    print(\"Valid data: bronze == silver valid + silver corrupted\")\n",
        "else:\n",
        "    print(\"Inconsistent: silver total data is not consistent with bronze data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/\")\n",
        "\n",
        "# Define window specification:\n",
        "# Partition by the columns that define duplicates (excluding timestamp),\n",
        "# and order by timestamp ascending to get the oldest record first\n",
        "window_spec = Window.partitionBy(\"message_id\", \"channel\", \"event_type\").orderBy(col(\"timestamp\").asc())\n",
        "\n",
        "# Add a row number column 'rn' to identify the oldest record within each partition\n",
        "df_ranked = df.withColumn(\"rn\", row_number().over(window_spec))\n",
        "\n",
        "# Filter to keep only the first record (oldest timestamp) per group and drop the helper column\n",
        "df_dedup = df_ranked.filter(col(\"rn\") == 1).drop(\"rn\")\n",
        "\n",
        "df_dedup.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w952WsbsOlqA",
        "outputId": "46972c5d-75f2-421d-9b38-7bce588043cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "|country_id|           timestamp|event_type|          message_id|channel|user_id|       country|      date|\n",
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "|      2004|2025-07-16 19:21:...|      OPEN|02cda1d4-d2ad-480...|   CHAT|   1035|        France|2025-07-16|\n",
            "|      2007|2025-07-16 19:21:...|      OPEN|02cda1d4-d2ad-480...|    SMS|   1032| United States|2025-07-16|\n",
            "|      2013|2025-07-16 19:20:...|      OPEN|1715b529-25d2-436...|   PUSH|   1011|   South Korea|2025-07-16|\n",
            "|      2011|2025-07-16 19:20:...|  RECEIVED|177052d8-e0e1-416...|   PUSH|   1000|         China|2025-07-16|\n",
            "|      2004|2025-07-16 19:20:...|  RECEIVED|1afac862-50c8-47b...|  EMAIL|   1022|        France|2025-07-16|\n",
            "|      2013|2025-07-16 19:20:...|  RECEIVED|2ea590be-6770-424...|  EMAIL|   1019|   South Korea|2025-07-16|\n",
            "|      2013|2025-07-16 19:20:...|      SENT|326588cf-6414-430...|   PUSH|   1028|   South Korea|2025-07-16|\n",
            "|      2014|2025-07-16 19:21:...|  RECEIVED|4a5236aa-bce7-478...|  OTHER|   1050|        Russia|2025-07-16|\n",
            "|      2015|2025-07-16 19:21:...|      SENT|4a5236aa-bce7-478...|    SMS|   1029|     Argentina|2025-07-16|\n",
            "|      2004|2025-07-16 19:21:...|      OPEN|4d9f7750-1f03-400...|   CHAT|   1028|        France|2025-07-16|\n",
            "|      2013|2025-07-16 19:21:...|  RECEIVED|4d9f7750-1f03-400...|   PUSH|   1045|   South Korea|2025-07-16|\n",
            "|      2007|2025-07-16 19:20:...|      SENT|4d9f7750-1f03-400...|    SMS|   1026| United States|2025-07-16|\n",
            "|      2014|2025-07-16 19:21:...|   CLICKED|62874f3a-c4cc-429...|   CHAT|   1046|        Russia|2025-07-16|\n",
            "|      2004|2025-07-16 19:21:...|   CREATED|6559a79c-2fed-43f...|   CHAT|   1011|        France|2025-07-16|\n",
            "|      2007|2025-07-16 19:20:...|  RECEIVED|6559a79c-2fed-43f...|    SMS|   1034| United States|2025-07-16|\n",
            "|      2008|2025-07-16 19:21:...|  RECEIVED|6615e9f3-2579-4fb...|    SMS|   1012|        Canada|2025-07-16|\n",
            "|      2000|2025-07-16 19:21:...|   CREATED|75574050-3627-480...|   PUSH|   1010|        Brazil|2025-07-16|\n",
            "|      2006|2025-07-16 19:21:...|   CREATED|793022e6-1ed9-4c3...|   CHAT|   1008|United Kingdom|2025-07-16|\n",
            "|      2012|2025-07-16 19:20:...|      SENT|793022e6-1ed9-4c3...|   PUSH|   1014|         India|2025-07-16|\n",
            "|      2015|2025-07-16 19:21:...|   CREATED|793022e6-1ed9-4c3...|    SMS|   1029|     Argentina|2025-07-16|\n",
            "+----------+--------------------+----------+--------------------+-------+-------+--------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c461b0c5-e51f-462a-fbc7-2f8ba4b651bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2025-07-16|  EMAIL|      2|      2|   2|       3|   2|\n",
            "|2025-07-16|    SMS|   NULL|      1|   3|       2|   2|\n",
            "|2025-07-16|   CHAT|      1|      3|   4|    NULL|   3|\n",
            "|2025-07-16|  OTHER|      1|      1|NULL|       2|NULL|\n",
            "|2025-07-16|   PUSH|   NULL|      2|   4|       2|   3|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Report 1\n",
        "df_dedup.groupBy(\"date\", \"channel\").pivot(\"event_type\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478d69ad-2e7a-498e-daf7-4def57e2ed8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-----+----+---+\n",
            "|user_id|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----+-----+-----+----+---+\n",
            "|   1005|   0|    1|    0|   0|  0|\n",
            "|   1031|   1|    0|    1|   0|  0|\n",
            "|   1034|   0|    0|    0|   0|  1|\n",
            "|   1030|   0|    0|    0|   0|  1|\n",
            "|   1019|   0|    1|    0|   0|  0|\n",
            "|   1046|   1|    0|    0|   0|  0|\n",
            "|   1008|   1|    0|    0|   1|  0|\n",
            "|   1021|   0|    2|    0|   0|  0|\n",
            "|   1026|   0|    0|    0|   0|  1|\n",
            "|   1028|   1|    0|    0|   1|  0|\n",
            "|   1029|   0|    0|    0|   0|  2|\n",
            "|   1032|   0|    0|    0|   0|  1|\n",
            "|   1010|   0|    1|    0|   1|  0|\n",
            "|   1048|   0|    0|    1|   0|  0|\n",
            "|   1050|   0|    0|    1|   0|  0|\n",
            "|   1035|   1|    0|    0|   0|  0|\n",
            "|   1045|   0|    0|    0|   1|  0|\n",
            "|   1017|   1|    0|    0|   0|  0|\n",
            "|   1022|   0|    1|    0|   0|  0|\n",
            "|   1015|   1|    1|    0|   0|  0|\n",
            "+-------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|   1005|         1|   0|    1|    0|   0|  0|\n",
            "|   1031|         2|   1|    0|    1|   0|  0|\n",
            "|   1034|         1|   0|    0|    0|   0|  1|\n",
            "|   1030|         1|   0|    0|    0|   0|  1|\n",
            "|   1019|         1|   0|    1|    0|   0|  0|\n",
            "|   1046|         1|   1|    0|    0|   0|  0|\n",
            "|   1008|         2|   1|    0|    0|   1|  0|\n",
            "|   1021|         2|   0|    2|    0|   0|  0|\n",
            "|   1026|         1|   0|    0|    0|   0|  1|\n",
            "|   1028|         2|   1|    0|    0|   1|  0|\n",
            "|   1029|         2|   0|    0|    0|   0|  2|\n",
            "|   1032|         1|   0|    0|    0|   0|  1|\n",
            "|   1010|         2|   0|    1|    0|   1|  0|\n",
            "|   1048|         1|   0|    0|    1|   0|  0|\n",
            "|   1050|         1|   0|    0|    1|   0|  0|\n",
            "|   1035|         1|   1|    0|    0|   0|  0|\n",
            "|   1045|         1|   0|    0|    0|   1|  0|\n",
            "|   1017|         1|   1|    0|    0|   0|  0|\n",
            "|   1022|         1|   0|    1|    0|   0|  0|\n",
            "|   1015|         2|   1|    1|    0|   0|  0|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Report 2\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "# Pivot the channels per user\n",
        "df_pivot = df_dedup.groupBy(\"user_id\").pivot(\"channel\").count().na.fill(0)\n",
        "\n",
        "df_pivot.show()\n",
        "\n",
        "# Add 'iterations' column (sum of all channel counts)\n",
        "channel_cols = df_pivot.columns[1:]\n",
        "\n",
        "df_result = df_pivot.withColumn(\n",
        "    \"iterations\", sum([col(c) for c in channel_cols])\n",
        ")\n",
        "\n",
        "df_result = df_result.select([\"user_id\", \"iterations\"] + channel_cols)\n",
        "\n",
        "df_result.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ef0RjFTxwE5y",
        "outputId": "8c06ef59-fdd2-4622-a98a-be8a5fcc51e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '—' (U+2014) (ipython-input-11-3105016965.py, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-11-3105016965.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    Para este caso de uso — em que se pretende uma agregação de dados em quase tempo real para alimentar um dashboard web com baixa latência (poucos minutos) — a minha sugestão seria manter o uso de Spark Structured Streaming.\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '—' (U+2014)\n"
          ]
        }
      ],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "\n",
        "Para este use case, em que se pretende uma agregação de dados em quase tempo real para alimentar um dashboard web com baixa latência (poucos minutos) — a minha sugestão seria manter o uso de Spark Structured Streaming.\n",
        "\n",
        "O Spark permite processar streams de dados de forma contínua, aplicar transformações complexas, operações de enriquecimento e agregações com janelas de tempo (windowing), que são essenciais para alimentar dashboards com métricas temporais (como contagens por minuto ou média de interações por canal num intervalo de tempo).\n",
        "\n",
        "No entanto, se o objetivo for ter uma latência ainda mais baixa (por exemplo, em segundos), ferramentas como Apache Flink poderiam ser mais adequadas, pois foram desenhadas com foco em processamento em tempo real com menor overhead.\n",
        "\n",
        "\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n",
        "Para alimentar diretamente um dashboard com agregações em tempo real, a escolha ideal seria uma base de dados analítica de leitura rápida, como por exemplo o BigQuery (exemplo dentro da Google Cloud)\n",
        "BigQuery é um data warehouse altamente escalável e otimizado para análises rápidas e agregações. É totalmente compatível com dashboards, suporta SQL e permite responder rapidamente a queries mesmo com grandes volumes de dados. É uma escolha natural para armazenamento de dados agregados por janelas de tempo.\n",
        "\n",
        "\n",
        "Um data lake seria mais indicado para o armazenamento de dados raw e não oferece performance suficiente para dashboards interativos.\n",
        "Requer também uma camada extra de processamento (Spark, Presto, etc.) antes que os dados possam ser consultados eficientemente — o que introduz latência e complexidade.\n",
        "\n",
        "\n",
        "O Kafka é uma ferramenta de transporte de eventos, excelente para ingestão de dados em tempo real.\n",
        "No entanto, não foi desenhado para servir dashboards — não suporta SQL nativo, não permite consultas ad hoc eficientes, e não guarda histórico a longo prazo de forma ideal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkBhbzVDz8Mk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}